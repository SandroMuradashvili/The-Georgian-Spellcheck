{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgbVFFf2EkDQmr8IjRp2j4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SandroMuradashvili/The-Georgian-Spellcheck/blob/main/interface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa_FuOBguCCy",
        "outputId": "72393094-72a8-44f2-9822-c9d5dd8d6a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading vocabulary...\n",
            "Vocabulary size: 37\n",
            "\n",
            "Loading trained model...\n",
            "Model loaded successfully!\n",
            "Validation loss: 0.2409\n",
            "\n",
            "======================================================================\n",
            "SPELLCHECK DEMONSTRATION\n",
            "======================================================================\n",
            "\n",
            "Input Word                Corrected Word            Status\n",
            "----------------------------------------------------------------------\n",
            "გამარჰობა                 გამარჰობაა                ✎ Corrected\n",
            "თბილსი                    თბილისი                   ✎ Corrected\n",
            "საქარტველო                საქარტველო                ✓ No change\n",
            "პროგამა                   როგამაა                   ✎ Corrected\n",
            "კომპიუტერი                კომპიუტერი                ✓ No change\n",
            "გაიარჯობა                 გაიარჯობაა                ✎ Corrected\n",
            "ქართუული                  ქართული                   ✎ Corrected\n",
            "უნივრსიტეტი               უნივრსიტეტი               ✓ No change\n",
            "დილამშვიდობისა            დილამშვიდობისა            ✓ No change\n",
            "მადლბა                    მადლბაა                   ✎ Corrected\n",
            "თბილისი                   თბილისი                   ✓ No change\n",
            "საქართველო                საქართველო                ✓ No change\n",
            "ბატონი                    ატონიი                    ✎ Corrected\n",
            "ქალბატონი                 ქალბატონი                 ✓ No change\n",
            "დღეს                      ღესს                      ✎ Corrected\n",
            "ხვალ                      ვალლ                      ✎ Corrected\n",
            "წიგნი                     იგნიი                     ✎ Corrected\n",
            "სკოლა                     კოლაა                     ✎ Corrected\n",
            "სახლი                     სახლი                     ✓ No change\n",
            "მანქანა                   მანქანაა                  ✎ Corrected\n",
            "\n",
            "======================================================================\n",
            "INTERACTIVE MODE\n",
            "======================================================================\n",
            "\n",
            "Enter Georgian words to correct (or 'quit' to exit):\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Georgian Spellcheck - Inference Notebook\n",
        "Load trained model and test corrections\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# SETUP & DEPENDENCIES\n",
        "# ============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "from typing import List\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD VOCABULARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Loading vocabulary...\")\n",
        "with open('vocab.pkl', 'rb') as f:\n",
        "    vocab_data = pickle.load(f)\n",
        "\n",
        "char_to_idx = vocab_data['char_to_idx']\n",
        "idx_to_char = vocab_data['idx_to_char']\n",
        "vocab_size = vocab_data['vocab_size']\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL ARCHITECTURE (Same as training)\n",
        "# ============================================================================\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=char_to_idx['<PAD>'])\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True,\n",
        "                           dropout=dropout if num_layers > 1 else 0, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return outputs, (hidden, cell)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hidden_dim * 3, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        return torch.softmax(attention, dim=1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=char_to_idx['<PAD>'])\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim + hidden_dim * 2, hidden_dim, num_layers,\n",
        "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, hidden, cell, encoder_outputs):\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
        "        attn_weights = attn_weights.unsqueeze(1)\n",
        "        context = torch.bmm(attn_weights, encoder_outputs)\n",
        "        lstm_input = torch.cat((embedded, context), dim=2)\n",
        "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "        prediction = self.fc(output.squeeze(1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2SeqSpellchecker(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(vocab_size, embed_dim, hidden_dim, num_layers, dropout)\n",
        "        self.decoder = Decoder(vocab_size, embed_dim, hidden_dim, num_layers, dropout)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
        "\n",
        "        hidden = hidden.view(self.num_layers, 2, batch_size, self.hidden_dim)\n",
        "        hidden = hidden.permute(0, 2, 1, 3).contiguous().view(self.num_layers, batch_size, -1)\n",
        "        hidden = hidden[:, :, :self.hidden_dim].contiguous()\n",
        "\n",
        "        cell = cell.view(self.num_layers, 2, batch_size, self.hidden_dim)\n",
        "        cell = cell.permute(0, 2, 1, 3).contiguous().view(self.num_layers, batch_size, -1)\n",
        "        cell = cell[:, :, :self.hidden_dim].contiguous()\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, self.decoder.vocab_size).to(src.device)\n",
        "        input_token = trg[:, 0].unsqueeze(1)\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input_token, hidden, cell, encoder_outputs)\n",
        "            outputs[:, t] = output\n",
        "            teacher_force = False  # No teacher forcing in inference\n",
        "            top1 = output.argmax(1).unsqueeze(1)\n",
        "            input_token = trg[:, t].unsqueeze(1) if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD TRAINED MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nLoading trained model...\")\n",
        "model = Seq2SeqSpellchecker(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=128,\n",
        "    hidden_dim=256,\n",
        "    num_layers=2,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "checkpoint = torch.load('best_model.pt', map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Validation loss: {checkpoint['val_loss']:.4f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# INFERENCE FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def correct_word(word: str, model_path: str = 'best_model.pt', max_length: int = 50) -> str:\n",
        "    \"\"\"\n",
        "    Takes a potentially misspelled Georgian word and returns the corrected version.\n",
        "\n",
        "    Args:\n",
        "        word: Input word (potentially misspelled)\n",
        "        model_path: Path to trained model (not used here since model already loaded)\n",
        "        max_length: Maximum output length\n",
        "\n",
        "    Returns:\n",
        "        Corrected word\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Convert word to indices\n",
        "        input_indices = [char_to_idx.get(c, char_to_idx['<UNK>']) for c in word]\n",
        "        input_tensor = torch.tensor([input_indices]).to(device)\n",
        "\n",
        "        # Encode\n",
        "        encoder_outputs, (hidden, cell) = model.encoder(input_tensor)\n",
        "\n",
        "        # Prepare decoder hidden states\n",
        "        batch_size = 1\n",
        "        hidden = hidden.view(model.num_layers, 2, batch_size, model.hidden_dim)\n",
        "        hidden = hidden.permute(0, 2, 1, 3).contiguous().view(model.num_layers, batch_size, -1)\n",
        "        hidden = hidden[:, :, :model.hidden_dim].contiguous()\n",
        "\n",
        "        cell = cell.view(model.num_layers, 2, batch_size, model.hidden_dim)\n",
        "        cell = cell.permute(0, 2, 1, 3).contiguous().view(model.num_layers, batch_size, -1)\n",
        "        cell = cell[:, :, :model.hidden_dim].contiguous()\n",
        "\n",
        "        # Start decoding with <SOS> token\n",
        "        decoder_input = torch.tensor([[char_to_idx['<SOS>']]]).to(device)\n",
        "\n",
        "        output_chars = []\n",
        "        for _ in range(max_length):\n",
        "            output, hidden, cell = model.decoder(decoder_input, hidden, cell, encoder_outputs)\n",
        "            predicted_idx = output.argmax(1).item()\n",
        "\n",
        "            # Check for <EOS> or <PAD>\n",
        "            if predicted_idx == char_to_idx['<EOS>'] or predicted_idx == char_to_idx['<PAD>']:\n",
        "                break\n",
        "\n",
        "            # Skip special tokens\n",
        "            if predicted_idx not in [char_to_idx['<SOS>'], char_to_idx['<UNK>']]:\n",
        "                output_chars.append(idx_to_char[predicted_idx])\n",
        "\n",
        "            decoder_input = torch.tensor([[predicted_idx]]).to(device)\n",
        "\n",
        "        return ''.join(output_chars)\n",
        "\n",
        "# ============================================================================\n",
        "# TEST EXAMPLES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SPELLCHECK DEMONSTRATION\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Test cases: mix of errors and correct words\n",
        "test_words = [\n",
        "    # Misspelled words (various error types)\n",
        "    \"გამარჰობა\",     # გამარჯობა (hello) - substitution\n",
        "    \"თბილსი\",        # თბილისი (Tbilisi) - deletion\n",
        "    \"საქარტველო\",    # საქართველო (Georgia) - substitution\n",
        "    \"პროგამა\",       # პროგრამა (program) - deletion\n",
        "    \"კომპიუტერი\",    # Already correct\n",
        "    \"გაიარჯობა\",     # გამარჯობა - substitution/insertion\n",
        "    \"ქართუული\",      # ქართული (Georgian) - insertion\n",
        "    \"უნივრსიტეტი\",  # უნივერსიტეტი (university) - substitution\n",
        "    \"დილამშვიდობისა\", # Already correct (good morning)\n",
        "    \"მადლბა\",        # მადლობა (thank you) - deletion\n",
        "    \"თბილისი\",       # Already correct (Tbilisi)\n",
        "    \"საქართველო\",    # Already correct (Georgia)\n",
        "    \"ბატონი\",        # Already correct (mister)\n",
        "    \"ქალბატონი\",     # Already correct (madam)\n",
        "    \"დღეს\",          # Already correct (today)\n",
        "    \"ხვალ\",          # Already correct (tomorrow)\n",
        "    \"წიგნი\",         # Already correct (book)\n",
        "    \"სკოლა\",         # Already correct (school)\n",
        "    \"სახლი\",         # Already correct (house)\n",
        "    \"მანქანა\",       # Already correct (car)\n",
        "]\n",
        "\n",
        "print(f\"{'Input Word':<25} {'Corrected Word':<25} {'Status'}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for word in test_words:\n",
        "    corrected = correct_word(word)\n",
        "    status = \"✓ No change\" if word == corrected else \"✎ Corrected\"\n",
        "    print(f\"{word:<25} {corrected:<25} {status}\")\n",
        "\n",
        "# ============================================================================\n",
        "# INTERACTIVE TESTING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INTERACTIVE MODE\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nEnter Georgian words to correct (or 'quit' to exit):\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter word: \").strip()\n",
        "\n",
        "    if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "        print(\"Exiting...\")\n",
        "        break\n",
        "\n",
        "    if not user_input:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        corrected = correct_word(user_input)\n",
        "        if user_input == corrected:\n",
        "            print(f\"Result: {corrected} (no correction needed)\")\n",
        "        else:\n",
        "            print(f\"Result: {user_input} → {corrected}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"\\nDone!\")"
      ]
    }
  ]
}